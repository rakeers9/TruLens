{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b38df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from trulens.core import Feedback, Select, TruSession\n",
    "from trulens.providers.openai import OpenAI as TruOpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c17280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported TruApp and instrument!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from trulens.apps.app import TruApp, instrument\n",
    "    print(\"‚úÖ Successfully imported TruApp and instrument!\")\n",
    "    TruCustomApp = TruApp\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è TruApp import failed: {e}\")\n",
    "    try:\n",
    "        from trulens.apps.basic import TruBasicApp\n",
    "        from trulens.apps.app import instrument\n",
    "        print(\"‚úÖ Using TruBasicApp as fallback\")\n",
    "        TruCustomApp = TruBasicApp\n",
    "    except ImportError as e2:\n",
    "        print(f\"‚ö†Ô∏è Both TruApp and TruBasicApp failed: {e2}\")\n",
    "        print(\"Using manual approach\")\n",
    "        TruCustomApp = None\n",
    "        def instrument(func):\n",
    "            return func\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c0ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ë Initialized with db url sqlite:///default.sqlite .\n",
      "üõë Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "tru = TruSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076a2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(openai, 'api_key'):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dea287",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\",\n",
    "        \"topic\": \"machine_learning\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"content\": \"Deep learning uses artificial neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "        \"topic\": \"deep_learning\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"content\": \"Natural language processing (NLP) is a branch of AI that helps computers understand, interpret and manipulate human language.\",\n",
    "        \"topic\": \"nlp\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"content\": \"Computer vision enables machines to interpret and make decisions based on visual information from the world.\",\n",
    "        \"topic\": \"computer_vision\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"content\": \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize reward.\",\n",
    "        \"topic\": \"reinforcement_learning\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "class SimpleRAGApp:\n",
    "    \n",
    "    def __init__(self, knowledge_base: List[Dict]):\n",
    "        self.knowledge_base = knowledge_base\n",
    "        \n",
    "    @instrument\n",
    "    def retrieve_context(self, query: str, top_k: int = 2) -> List[str]:\n",
    "        query_lower = query.lower()\n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in self.knowledge_base:\n",
    "            content_words = set(doc['content'].lower().split())\n",
    "            query_words = set(query_lower.split())\n",
    "            overlap = len(content_words.intersection(query_words))\n",
    "            \n",
    "            if overlap > 0:\n",
    "                scored_docs.append((doc['content'], overlap))\n",
    "        \n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc[0] for doc in scored_docs[:top_k]]\n",
    "    \n",
    "    @instrument\n",
    "    def generate_answer(self, query: str, context: List[str]) -> str:\n",
    "        context_text = \"\\n\\n\".join(context)\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following context, please answer the question.\n",
    "        \n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Answer questions based on the provided context.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    @instrument\n",
    "    def query(self, question: str) -> str:\n",
    "        context = self.retrieve_context(question)\n",
    "        \n",
    "        answer = self.generate_answer(question, context)\n",
    "        \n",
    "        return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feda364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_app = SimpleRAGApp(knowledge_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c36394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is machine learning?\n",
      "Generated Answer: Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"What is machine learning?\"\n",
    "result = rag_app.query(test_question)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Generated Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc1d7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ In Groundedness, input source will be set to __record__.app.retrieve_context.rets.collect() .\n",
      "‚úÖ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "‚úÖ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "‚úÖ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "‚úÖ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "‚úÖ In Context Relevance, input context will be set to __record__.app.retrieve_context.rets.collect() .\n"
     ]
    }
   ],
   "source": [
    "provider = TruOpenAI()\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, \n",
    "        name=\"Groundedness\"\n",
    "    )\n",
    "    .on(Select.RecordCalls.retrieve_context.rets.collect()) #context\n",
    "    .on_output() # actual output\n",
    ")\n",
    "\n",
    "f_answer_relevance = (\n",
    "    Feedback(\n",
    "        provider.relevance_with_cot_reasons, \n",
    "        name=\"Answer Relevance\"\n",
    "    )\n",
    "    .on_input() #question\n",
    "    .on_output() #AI response\n",
    ")\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, \n",
    "        name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input() #question\n",
    "    .on(Select.RecordCalls.retrieve_context.rets.collect()) #context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb45826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feedback implementation <function completeness_feedback at 0x172d0db20> cannot be serialized: Module __main__ is not importable. This may be ok unless you are using the deferred feedback mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ In Completeness, input input_text will be set to __record__.main_input or `Select.RecordInput` .\n",
      "‚úÖ In Completeness, input output_text will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "def completeness_feedback(input_text: str, output_text: str) -> float:\n",
    "    \"\"\"\n",
    "    measures if answer addressses question\n",
    "    \"\"\"\n",
    "    question_words = set(input_text.lower().split())\n",
    "    answer_words = set(output_text.lower().split())\n",
    "    \n",
    "    # Remove common stop words\n",
    "    stop_words = {'what', 'is', 'the', 'how', 'why', 'when', 'where', 'a', 'an', 'and', 'or', 'but'}\n",
    "    question_words = question_words - stop_words\n",
    "    \n",
    "    if not question_words:\n",
    "        return 0.5\n",
    "    \n",
    "    overlap = len(question_words.intersection(answer_words))\n",
    "    return min(1.0, overlap / len(question_words))\n",
    "\n",
    "f_completeness = Feedback(completeness_feedback, name=\"Completeness\").on_input().on_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92085a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class '__main__.SimpleRAGApp'> for base <class '__main__.SimpleRAGApp'>\n",
      "\tinstrumenting retrieve_context\n",
      "\tinstrumenting generate_answer\n",
      "\tinstrumenting query\n",
      "üîó TruApp wrapper created successfully!\n",
      "   App name: SimpleRAG_Demo\n",
      "   App version: 2025_v1\n",
      "   Feedbacks: 4\n",
      "üìä Ready to run evaluations!\n"
     ]
    }
   ],
   "source": [
    "if TruCustomApp is not None:\n",
    "    try:\n",
    "        tru_rag = TruCustomApp(\n",
    "            rag_app,\n",
    "            app_name='SimpleRAG_Demo', \n",
    "            app_version='2025_v1',\n",
    "            feedbacks=[\n",
    "                f_groundedness,\n",
    "                f_answer_relevance,\n",
    "                f_context_relevance,\n",
    "                f_completeness\n",
    "            ]\n",
    "        )\n",
    "        print(\"üîó TruApp wrapper created successfully!\")\n",
    "        print(f\"   App name: SimpleRAG_Demo\")\n",
    "        print(f\"   App version: 2025_v1\")\n",
    "        print(f\"   Feedbacks: {len([f_groundedness, f_answer_relevance, f_context_relevance, f_completeness])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è TruApp creation failed: {e}\")\n",
    "        print(\"Falling back to manual approach\")\n",
    "        TruCustomApp = None\n",
    "\n",
    "if TruCustomApp is None:\n",
    "    print(\"‚ö†Ô∏è Using manual instrumentation approach\")\n",
    "    \n",
    "    class SimpleRAGWrapper:\n",
    "        def __init__(self, app):\n",
    "            self.app = app\n",
    "            self.app_name = 'SimpleRAG_Demo'\n",
    "            self.app_version = '2025_v1'\n",
    "        \n",
    "        def query(self, question: str):\n",
    "            return self.app.query(question)\n",
    "    \n",
    "    tru_rag = SimpleRAGWrapper(rag_app)\n",
    "    print(\"üîó Simple wrapper created!\")\n",
    "\n",
    "print(\"üìä Ready to run evaluations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c4e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does deep learning work?\", \n",
    "    \"What is the difference between machine learning and deep learning?\",\n",
    "    \"Can you explain natural language processing?\",\n",
    "    \"What are the applications of computer vision?\",\n",
    "    \"How does reinforcement learning work?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "]\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21863b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using full TruApp integration!\n",
      "\n",
      "üìù Processing: What is machine learning?\n",
      "‚úÖ Answer: Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n",
      "üìù Processing: How does deep learning work?\n",
      "‚úÖ Answer: Deep learning works by using artificial neural networks with multiple layers to model and understand...\n",
      "\n",
      "üìù Processing: What is the difference between machine learning and deep learning?\n",
      "‚úÖ Answer: The main difference between machine learning and deep learning is that deep learning is a subset of ...\n",
      "\n",
      "üìù Processing: Can you explain natural language processing?\n",
      "‚úÖ Answer: Natural language processing (NLP) is a branch of artificial intelligence (AI) that focuses on enabli...\n",
      "\n",
      "üìù Processing: What are the applications of computer vision?\n",
      "‚úÖ Answer: Some applications of computer vision include facial recognition, object detection, image classificat...\n",
      "\n",
      "üìù Processing: How does reinforcement learning work?\n",
      "‚úÖ Answer: Reinforcement learning works by having an agent learn to make decisions through trial and error in a...\n",
      "\n",
      "üìù Processing: What is artificial intelligence?\n",
      "‚úÖ Answer: Artificial intelligence (AI) is a broad field of computer science that focuses on creating machines ...\n"
     ]
    }
   ],
   "source": [
    "if hasattr(tru_rag, 'app_name') and hasattr(tru_rag, 'feedbacks'):\n",
    "    print(\"üöÄ Using full TruApp integration!\")\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nüìù Processing: {question}\")\n",
    "        \n",
    "        # Automatically trigger TruLens evaluation\n",
    "        try:\n",
    "            with tru_rag as recording:\n",
    "                if hasattr(tru_rag, 'app'):\n",
    "                    answer = tru_rag.app.query(question)\n",
    "                else:\n",
    "                    # For TruBasicApp, the app itself is callable\n",
    "                    answer = tru_rag(question)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è TruApp context failed: {e}\")\n",
    "            # Fallback to direct query\n",
    "            try:\n",
    "                if hasattr(tru_rag, 'app'):\n",
    "                    answer = tru_rag.app.query(question)\n",
    "                else:\n",
    "                    answer = tru_rag(question)\n",
    "            except:\n",
    "                answer = rag_app.query(question)  # Ultimate fallback\n",
    "            \n",
    "        print(f\"‚úÖ Answer: {answer[:100]}...\")\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'answer': answer\n",
    "        })\n",
    "        \n",
    "else:\n",
    "\n",
    "    print(\"Manual\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfd50d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 14 records from TruLens database (all apps)\n",
      "Found 14 total records, looking for SimpleRAG_Demo records...\n",
      "Apps in database by app_name: ['SimpleRAG_Demo']\n",
      "After filtering: 14 records found\n",
      "EXPLICIT DEBUG FINAL: About to process 14 records\n",
      "EXPLICIT DEBUG FINAL: has_trulens_records = True\n",
      "EXPLICIT DEBUG FINAL: records.empty = False\n",
      "üìà Evaluation Results Summary:\n",
      "==================================================\n",
      "Total TruLens evaluations: 14\n",
      "Feedback score columns found: []\n",
      "Cost columns found: ['Groundedness feedback cost in USD', 'Answer Relevance feedback cost in USD', 'Context Relevance feedback cost in USD', 'Completeness feedback cost in USD']\n",
      "\n",
      "‚ö†Ô∏è No feedback scores found, only cost information.\n",
      "This suggests the feedback functions didn't complete evaluation.\n",
      "\n",
      "üîç Record columns: ['app_name', 'app_version', 'app_id', 'app_json', 'type', 'record_id', 'input', 'output', 'tags', 'record_json', 'cost_json', 'perf_json', 'ts', 'Groundedness', 'Answer Relevance', 'Context Relevance', 'Completeness', 'Groundedness_calls', 'Answer Relevance_calls', 'Context Relevance_calls', 'Completeness_calls', 'Groundedness feedback cost in USD', 'Answer Relevance feedback cost in USD', 'Context Relevance feedback cost in USD', 'Completeness feedback cost in USD', 'latency', 'total_tokens', 'total_cost', 'cost_currency']\n",
      "\n",
      "6. Testing OpenAI connectivity with manual feedback evaluation...\n",
      "   Manual completeness score: 0.500 (This works)\n",
      "   Testing OpenAI-based feedback...\n",
      "   Manual relevance score: 1.000 (OpenAI working)\n",
      "\n",
      "üìã Sample record structure:\n",
      "  app_name: SimpleRAG_Demo\n",
      "  app_version: 2025_v1\n",
      "  app_id: app_hash_58e2ab7b59aac54a1bf1c0590b7ae305\n",
      "  type: SimpleRAGApp(__main__)\n",
      "  record_id: record_hash_21ddf5b5dafe71aef037f9d61e0ba959\n",
      "  input: What is artificial intelligence?\n",
      "  output: Artificial intelligence (AI) is a broad field of computer science that focuses on creating machines ...\n",
      "  tags: -\n",
      "  cost_json: {\"n_requests\": 1, \"n_successful_requests\": 1, \"n_completion_requests\": 1, \"n_classification_requests...\n",
      "  perf_json: {\"start_time\": \"2025-08-26T09:07:56.928109\", \"end_time\": \"2025-08-26T09:07:57.861098\"}\n",
      "  ts: 2025-08-26T09:07:57.861361\n",
      "  Groundedness: 0.6666666666666666\n",
      "  Answer Relevance: 1.0\n",
      "  Context Relevance: 0.3333333333333333\n",
      "  Completeness: 0.5\n",
      "  Groundedness_calls: [{'args': {'source': [['Machine learning is a subset of artificial intelligence that enables compute...\n",
      "  Answer Relevance_calls: [{'args': {'prompt': 'What is artificial intelligence?', 'response': 'Artificial intelligence (AI) i...\n",
      "  Context Relevance_calls: [{'args': {'question': 'What is artificial intelligence?', 'context': [['Machine learning is a subse...\n",
      "  Completeness_calls: [{'args': {'input_text': 'What is artificial intelligence?', 'output_text': 'Artificial intelligence...\n",
      "  Groundedness feedback cost in USD: 4.515e-05\n",
      "  Answer Relevance feedback cost in USD: 0.0\n",
      "  Context Relevance feedback cost in USD: 0.0\n",
      "  Completeness feedback cost in USD: 0.0\n",
      "  latency: 0.932989\n",
      "  total_tokens: 135\n",
      "  total_cost: 0.000225\n",
      "  cost_currency: USD\n",
      "\n",
      "üî¨ Diagnosis:\n",
      "- Issue: Feedback functions aren't producing scores\n",
      "- Status: Functions are being invoked (costs tracked) but not completing\n",
      "\n",
      "Debugging steps:\n",
      "1. Checking if evaluations are in deferred mode...\n",
      "2. Attempting to run deferred evaluations...\n",
      "3. Evaluation attempt failed: Evaluator is already running in this process.\n",
      "4. Feedback mode: FeedbackMode.WITH_APP_THREAD\n",
      "5. Possible causes:\n",
      "   - OpenAI API rate limits or permissions\n",
      "   - Network connectivity issues\n",
      "   - Feedback functions timing out\n",
      "   - Evaluation running in background (deferred mode)\n",
      "\n",
      "üìä Detailed Results:\n",
      "------------------------------\n",
      "\n",
      "Question 1:\n",
      "  Input: What is artificial intelligence?...\n",
      "  Output: Artificial intelligence (AI) is a broad field of computer science that focuses o...\n",
      "  Status: Feedback scores not available (see diagnosis above)\n",
      "\n",
      "Question 2:\n",
      "  Input: How does reinforcement learning work?...\n",
      "  Output: Reinforcement learning works by having an agent learn to make decisions through ...\n",
      "  Status: Feedback scores not available (see diagnosis above)\n",
      "\n",
      "Question 3:\n",
      "  Input: What are the applications of computer vision?...\n",
      "  Output: Some applications of computer vision include facial recognition, object detectio...\n",
      "  Status: Feedback scores not available (see diagnosis above)\n",
      "\n",
      "üîç Performance Analysis:\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try multiple API parameter variations to find the right one\n",
    "    try:\n",
    "        # Try without parameters first (gets all records)\n",
    "        records, feedback_results = tru.get_records_and_feedback()\n",
    "        print(f\"‚úÖ Retrieved {len(records)} records from TruLens database (all apps)\")\n",
    "    except:\n",
    "        # Try with app_id parameter (most common)\n",
    "        records, feedback_results = tru.get_records_and_feedback(app_ids=['SimpleRAG_Demo'])\n",
    "        print(f\"‚úÖ Retrieved {len(records)} records using app_ids parameter\")\n",
    "    \n",
    "    has_trulens_records = True\n",
    "    \n",
    "    # *** EXPLICIT CHANGE: Fixed filtering logic to find correct app records ***\n",
    "    # Filter records if we got more than expected\n",
    "    if hasattr(tru_rag, 'app_name') and len(records) > 7:\n",
    "        print(f\"Found {len(records)} total records, looking for SimpleRAG_Demo records...\")\n",
    "        \n",
    "        # Check what app names/IDs are actually in the database\n",
    "        if 'app_name' in records.columns:\n",
    "            unique_apps = records['app_name'].unique()\n",
    "            print(f\"Apps in database by app_name: {unique_apps}\")\n",
    "            # Filter by app_name column\n",
    "            records = records[records['app_name'] == 'SimpleRAG_Demo']\n",
    "            \n",
    "        elif 'app_json' in records.columns:\n",
    "            print(\"Checking app_json column for app identifiers...\")\n",
    "            # Look at a few app_json entries to see the structure\n",
    "            sample_apps = records['app_json'].head(3)\n",
    "            for i, app_json in enumerate(sample_apps):\n",
    "                print(f\"Sample app_json {i+1}: {str(app_json)[:200]}...\")\n",
    "            \n",
    "            # Try broader filtering - look for any of our possible names\n",
    "            possible_names = ['SimpleRAG_Demo', 'SimpleRAG', 'RAG']\n",
    "            mask = pd.Series([False] * len(records))\n",
    "            \n",
    "            for name in possible_names:\n",
    "                mask = mask | records['app_json'].str.contains(name, na=False)\n",
    "            \n",
    "            records = records[mask]\n",
    "            \n",
    "        elif 'record_id' in records.columns:\n",
    "            # If no app filtering available, just take the most recent 7 records\n",
    "            print(\"No app name column found, taking most recent 7 records\")\n",
    "            records = records.tail(7)\n",
    "        \n",
    "        print(f\"After filtering: {len(records)} records found\")\n",
    "        \n",
    "        # If filtering resulted in 0 records, don't filter - use all records\n",
    "        if len(records) == 0:\n",
    "            print(\"Filtering resulted in 0 records, using all records instead\")\n",
    "            records, _ = tru.get_records_and_feedback()  # Get all records again\n",
    "            print(f\"EXPLICIT DEBUG: After fallback, got {len(records)} records\")\n",
    "            print(f\"EXPLICIT DEBUG: has_trulens_records = {has_trulens_records}\")\n",
    "            print(f\"EXPLICIT DEBUG: records.empty = {records.empty}\")\n",
    "    \n",
    "    # EXPLICIT DEBUG: Check final state before proceeding to results\n",
    "    print(f\"EXPLICIT DEBUG FINAL: About to process {len(records)} records\")\n",
    "    print(f\"EXPLICIT DEBUG FINAL: has_trulens_records = {has_trulens_records}\")\n",
    "    print(f\"EXPLICIT DEBUG FINAL: records.empty = {records.empty}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not get TruLens records: {e}\")\n",
    "    print(\"Let's check what methods are available on the TruSession object\")\n",
    "    available_methods = [method for method in dir(tru) if 'record' in method.lower()]\n",
    "    print(f\"Available record methods: {available_methods}\")\n",
    "    \n",
    "    records = pd.DataFrame()  # Empty dataframe\n",
    "    feedback_results = pd.DataFrame()  \n",
    "    has_trulens_records = False\n",
    "\n",
    "print(\"üìà Evaluation Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if has_trulens_records and not records.empty:\n",
    "    # Display TruLens results\n",
    "    print(f\"Total TruLens evaluations: {len(records)}\")\n",
    "    \n",
    "    # Look for actual feedback score columns (not just cost columns)\n",
    "    feedback_cols = []\n",
    "    cost_cols = []\n",
    "    \n",
    "    for col in records.columns:\n",
    "        if 'feedback' in col.lower():\n",
    "            if 'cost' in col.lower():\n",
    "                cost_cols.append(col)\n",
    "            else:\n",
    "                feedback_cols.append(col)\n",
    "    \n",
    "    print(f\"Feedback score columns found: {feedback_cols}\")\n",
    "    print(f\"Cost columns found: {cost_cols}\")\n",
    "    \n",
    "    # Check if we have actual scores\n",
    "    if feedback_cols:\n",
    "        print(\"\\nüìä Feedback Scores:\")\n",
    "        for col in feedback_cols:\n",
    "            if col in records.columns and records[col].notna().any():\n",
    "                avg_score = records[col].mean()\n",
    "                print(f\"{col}: {avg_score:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No feedback scores found, only cost information.\")\n",
    "        print(\"This suggests the feedback functions didn't complete evaluation.\")\n",
    "        \n",
    "    # Check what's actually in the records\n",
    "    print(f\"\\nüîç Record columns: {list(records.columns)}\")\n",
    "    \n",
    "    # *** EXPLICIT CHANGE: Added manual feedback evaluation check ***\n",
    "    # Let's also try running one feedback function manually to test OpenAI connectivity\n",
    "    if not feedback_cols:\n",
    "        print(\"\\n6. Testing OpenAI connectivity with manual feedback evaluation...\")\n",
    "        try:\n",
    "            # Get a sample question and answer from our results\n",
    "            test_question = results[0]['question'] if results else \"What is machine learning?\"\n",
    "            test_answer = results[0]['answer'] if results else \"Machine learning is AI.\"\n",
    "            \n",
    "            # Try running our completeness feedback function\n",
    "            manual_score = completeness_feedback(test_question, test_answer)\n",
    "            print(f\"   Manual completeness score: {manual_score:.3f} (This works)\")\n",
    "            \n",
    "            # Try running an OpenAI-based feedback function manually\n",
    "            if 'provider' in locals():\n",
    "                print(\"   Testing OpenAI-based feedback...\")\n",
    "                try:\n",
    "                    # Simple relevance test\n",
    "                    relevance_score = provider.relevance(test_question, test_answer)\n",
    "                    print(f\"   Manual relevance score: {relevance_score:.3f} (OpenAI working)\")\n",
    "                except Exception as openai_error:\n",
    "                    print(f\"   OpenAI feedback failed: {openai_error}\")\n",
    "                    print(\"   This suggests an issue with the OpenAI provider setup\")\n",
    "            \n",
    "        except Exception as manual_error:\n",
    "            print(f\"   Manual evaluation failed: {manual_error}\")\n",
    "\n",
    "    # Show a sample record structure\n",
    "    if len(records) > 0:\n",
    "        print(f\"\\nüìã Sample record structure:\")\n",
    "        sample_record = records.iloc[0]\n",
    "        for col in sample_record.index:\n",
    "            if pd.notna(sample_record[col]) and col not in ['app_json', 'record_json']:\n",
    "                val = str(sample_record[col])[:100] + \"...\" if len(str(sample_record[col])) > 100 else sample_record[col]\n",
    "                print(f\"  {col}: {val}\")\n",
    "    \n",
    "    # *** EXPLICIT CHANGE: Added detailed debugging section ***\n",
    "    # Check if feedback functions are running\n",
    "    print(f\"\\nüî¨ Diagnosis:\")\n",
    "    if not feedback_cols:\n",
    "        print(\"- Issue: Feedback functions aren't producing scores\")\n",
    "        print(\"- Status: Functions are being invoked (costs tracked) but not completing\")\n",
    "        \n",
    "        # Check if evaluations are pending/deferred\n",
    "        print(\"\\nDebugging steps:\")\n",
    "        print(\"1. Checking if evaluations are in deferred mode...\")\n",
    "        \n",
    "        # Check the TruSession for any pending evaluations\n",
    "        try:\n",
    "            # Try to run any deferred feedback evaluations\n",
    "            print(\"2. Attempting to run deferred evaluations...\")\n",
    "            tru.start_evaluator()  # This starts the evaluation process\n",
    "            import time\n",
    "            time.sleep(5)  # Wait a bit for evaluations to complete\n",
    "            \n",
    "            # Try to get records again\n",
    "            records_retry, feedback_retry = tru.get_records_and_feedback()\n",
    "            \n",
    "            # Check for score columns again\n",
    "            feedback_cols_retry = [col for col in records_retry.columns if 'feedback' in col.lower() and 'cost' not in col.lower()]\n",
    "            \n",
    "            if feedback_cols_retry:\n",
    "                print(\"‚úÖ Deferred evaluations completed!\")\n",
    "                records = records_retry  # Update records\n",
    "                feedback_cols = feedback_cols_retry  # Update feedback columns\n",
    "            else:\n",
    "                print(\"‚ùå Deferred evaluations still not producing scores\")\n",
    "                \n",
    "        except Exception as eval_error:\n",
    "            print(f\"3. Evaluation attempt failed: {eval_error}\")\n",
    "            \n",
    "        # Additional diagnostic: Check feedback mode\n",
    "        if hasattr(tru_rag, 'feedback_mode'):\n",
    "            print(f\"4. Feedback mode: {tru_rag.feedback_mode}\")\n",
    "        \n",
    "        print(\"5. Possible causes:\")\n",
    "        print(\"   - OpenAI API rate limits or permissions\")\n",
    "        print(\"   - Network connectivity issues\")\n",
    "        print(\"   - Feedback functions timing out\")\n",
    "        print(\"   - Evaluation running in background (deferred mode)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"- Status: Feedback functions are working correctly!\")\n",
    "        \n",
    "    print(\"\\nüìä Detailed Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Show results for each question (limit to first 3 for readability)\n",
    "    for idx, row in records.head(3).iterrows():\n",
    "        print(f\"\\nQuestion {idx + 1}:\")\n",
    "        if 'input' in row:\n",
    "            print(f\"  Input: {str(row['input'])[:80]}...\")\n",
    "        if 'output' in row:\n",
    "            print(f\"  Output: {str(row['output'])[:80]}...\")\n",
    "        \n",
    "        # Show actual feedback scores if they exist\n",
    "        for col in feedback_cols:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                print(f\"  {col}: {row[col]:.3f}\")\n",
    "        \n",
    "        # If no scores, show what we have\n",
    "        if not feedback_cols:\n",
    "            print(\"  Status: Feedback scores not available (see diagnosis above)\")\n",
    "            \n",
    "else:\n",
    "    # Show basic results without full TruLens integration  \n",
    "    print(\"üìä Basic Evaluation Results:\")\n",
    "    print(f\"Total questions processed: {len(results)}\")\n",
    "    print(\"\\nSample questions and answers:\")\n",
    "    \n",
    "    for i, result in enumerate(results[:3]):  # Show first 3\n",
    "        print(f\"\\n{i+1}. Question: {result['question']}\")\n",
    "        print(f\"   Answer: {result['answer'][:100]}...\")\n",
    "        \n",
    "        # If we ran manual evaluations, show those results\n",
    "        if 'feedback_results' in locals():\n",
    "            print(f\"   Manual evaluation available\")\n",
    "    \n",
    "    print(f\"\\nüí° To get full TruLens integration:\")\n",
    "    print(\"1. Ensure your OpenAI API key is set correctly\")\n",
    "    print(\"2. Check that all TruLens packages are installed\") \n",
    "    print(\"3. Try running the notebook again\")\n",
    "    \n",
    "    # Create a simple summary\n",
    "    print(f\"\\nüìã Summary:\")\n",
    "    print(f\"  Questions processed: {len(results)}\")\n",
    "    print(f\"  All questions received responses: {'‚úÖ' if all('answer' in r for r in results) else '‚ùå'}\")\n",
    "    print(f\"  Average answer length: {np.mean([len(r['answer']) for r in results]):.0f} characters\")\n",
    "\n",
    "## Advanced Analysis Examples\n",
    "\n",
    "def analyze_performance_trends():\n",
    "    \"\"\"Analyze performance trends across different question types\"\"\"\n",
    "    if records.empty:\n",
    "        print(\"No data available for trend analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîç Performance Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Group by question type (simple keyword analysis)\n",
    "    question_types = []\n",
    "    for question in test_questions[:len(records)]:  # Match length with records\n",
    "        if 'what is' in question.lower():\n",
    "            question_types.append('Definition')\n",
    "        elif 'how does' in question.lower():\n",
    "            question_types.append('Explanation')\n",
    "        elif 'difference' in question.lower():\n",
    "            question_types.append('Comparison')\n",
    "        else:\n",
    "            question_types.append('Other')\n",
    "    \n",
    "    # Add question types to results\n",
    "    if len(question_types) == len(records):\n",
    "        records['question_type'] = question_types\n",
    "        \n",
    "        # Analyze by question type\n",
    "        feedback_metrics = [col for col in records.columns if 'feedback' in col.lower()]\n",
    "        \n",
    "        for qtype in set(question_types):\n",
    "            subset = records[records['question_type'] == qtype]\n",
    "            if not subset.empty:\n",
    "                print(f\"\\n{qtype} Questions (n={len(subset)}):\")\n",
    "                \n",
    "                for metric in feedback_metrics:\n",
    "                    if metric in subset.columns and subset[metric].notna().any():\n",
    "                        avg_score = subset[metric].mean()\n",
    "                        print(f\"  {metric}: {avg_score:.3f}\")\n",
    "\n",
    "analyze_performance_trends()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe70f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fb83750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Performance Analysis:\n",
      "========================================\n",
      "\n",
      "Explanation Questions (n=2):\n",
      "  Groundedness feedback cost in USD: 0.000\n",
      "  Answer Relevance feedback cost in USD: 0.000\n",
      "  Context Relevance feedback cost in USD: 0.000\n",
      "  Completeness feedback cost in USD: 0.000\n",
      "\n",
      "Definition Questions (n=3):\n",
      "  Groundedness feedback cost in USD: 0.000\n",
      "  Answer Relevance feedback cost in USD: 0.000\n",
      "  Context Relevance feedback cost in USD: 0.000\n",
      "  Completeness feedback cost in USD: 0.000\n",
      "\n",
      "Other Questions (n=2):\n",
      "  Groundedness feedback cost in USD: 0.000\n",
      "  Answer Relevance feedback cost in USD: 0.000\n",
      "  Context Relevance feedback cost in USD: 0.000\n",
      "  Completeness feedback cost in USD: 0.000\n"
     ]
    }
   ],
   "source": [
    "def analyze_performance_trends():\n",
    "    \"\"\"Analyze performance trends across different question types\"\"\"\n",
    "    if records.empty:\n",
    "        print(\"No data available for trend analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîç Performance Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Group by question type (simple keyword analysis)\n",
    "    question_types = []\n",
    "    for question in test_questions[:len(records)]:  # Match length with records\n",
    "        if 'what is' in question.lower():\n",
    "            question_types.append('Definition')\n",
    "        elif 'how does' in question.lower():\n",
    "            question_types.append('Explanation')\n",
    "        elif 'difference' in question.lower():\n",
    "            question_types.append('Comparison')\n",
    "        else:\n",
    "            question_types.append('Other')\n",
    "    \n",
    "    # Add question types to results\n",
    "    if len(question_types) == len(records):\n",
    "        records['question_type'] = question_types\n",
    "        \n",
    "        # Analyze by question type\n",
    "        feedback_metrics = [col for col in records.columns if 'feedback' in col.lower()]\n",
    "        \n",
    "        for qtype in set(question_types):\n",
    "            subset = records[records['question_type'] == qtype]\n",
    "            if not subset.empty:\n",
    "                print(f\"\\n{qtype} Questions (n={len(subset)}):\")\n",
    "                \n",
    "                for metric in feedback_metrics:\n",
    "                    if metric in subset.columns and subset[metric].notna().any():\n",
    "                        avg_score = subset[metric].mean()\n",
    "                        print(f\"  {metric}: {avg_score:.3f}\")\n",
    "\n",
    "analyze_performance_trends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d68bdb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key set: True\n",
      "Key preview: sk-proj-sb...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"OpenAI API Key set:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Key preview:\", os.getenv(\"OPENAI_API_KEY\", \"NOT_SET\")[:10] + \"...\" if os.getenv(\"OPENAI_API_KEY\") else \"NOT_SET\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a7610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
